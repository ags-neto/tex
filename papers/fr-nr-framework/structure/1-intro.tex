\section{Introduction}

The evaluation of image quality is essential in applications such as biometric authentication, multimedia processing, and medical imaging~\cite{Kim2015, Huang2020}. In the specific domain of Facial Image Quality Assessment (FIQA), the goal is to ensure that images used in face recognition systems meet a quality standard that optimizes recognition performance~\cite{Cavazos2021}. Unlike general Image Quality Assessment (IQA), which considers visual attributes such as contrast, sharpness, and noise, FIQA focuses on assessing image quality in a manner that directly impacts face recognition accuracy~\cite{Terhoerst2020}.

IQA methods are divided into two types: subjective and objective. Subjective IQA relies on human evaluators who assign quality scores based on perceived visual qualityâ€”typically aggregated as Mean Opinion Scores (MOS). While considered the gold standard, subjective evaluation is costly, time-consuming, and not scalable. In contrast, objective IQA methods use algorithmic models to predict perceptual quality automatically, often by pixel-wise comparison or learned features.

Objective IQA methods can be further categorized as Full-Reference (FR-IQA) or No-Reference (NR-IQA). FR-IQA requires access to an undistorted reference image and evaluates quality by comparing the distorted image to it. Although often accurate, FR-IQA is only applicable when a pristine reference is available. NR-IQA, on the other hand, estimates image quality without any reference, making it more suitable for real-world scenarios. However, its ability to generalize across content and distortion types remains limited.

Facial Image Quality Assessment (FIQA) falls within the broader IQA domain but is uniquely constrained by biometric applications, where reference images are rarely available. As such, most FIQA methods are inherently no-reference, relying on task-specific priors and learned representations to estimate quality.

A fundamental challenge in IQA lies in the discrepancy between objective quality metrics and human perceptual judgments. While classical metrics such as Peak Signal-to-Noise Ratio (PSNR)~\cite{gonzalez2002digital}, Structural Similarity Index (SSIM)~\cite{wang2004image}, and Visual Information Fidelity (VIF)~\cite{sheikh2006image} provide automated assessments of image quality, their correlation with subjective perception remains inconsistent across datasets~\cite{Babnik2022}. This misalignment is particularly problematic in facial image analysis, where perceptual quality is influenced by both intrinsic distortions and observer-related biases.

A growing body of research highlights the presence of demographic and non-demographic biases in FIQA, whereby factors such as ethnicity, gender, and age influence perceived image quality~\cite{Kabbani2024, 9304865, Cavazos2021, tapia2024beauty}. These biases often stem from dataset imbalances and inter-observer variability. For instance, face recognition accuracy tends to be lower for dark-skinned individuals, and female faces are frequently rated with lower quality scores in FIQA evaluations~\cite{Huang2020}, underscoring the need for inclusive and perceptually aligned quality assessment approaches.

The International Civil Aviation Organization~\cite{icao_2015} (ICAO) and the ISO/IEC 19794--5 standard~\cite{iso_iec29794-5_2010} establish guidelines for image quality in Machine-Readable Travel Documents (MRTDs). These guidelines ensure uniform image conditions (e.g., lighting, focus, and resolution) and consistency across datasets. While these regulations establish a technical baseline, they do not account for perceptual biases and demographic variability in FIQA.\@

The ethical implications of these biases are profound. Political regulations, such as the European Convention on Human Rights (Article 14)~\cite{echr_article14}, the Universal Declaration of Human Rights (Article 7)~\cite{udhr_article7}, the General Data Protection Regulation (Article 22)~\cite{gdpr_article22}, and emerging AI governance frameworks, such as the European Artificial Intelligence Act (2024)~\cite{eu_ai_act_2024} and proposals in the USA~\cite{us_ai_bill_rights_2022}, aim to prevent discriminatory decisions. Despite these efforts, biases persist, often introduced through the human observers who evaluate facial images for FIQA algorithms.

Evidence from neuroscience further supports the complexity of facial image perception. The fusiform face area, a specialized region in the human brain, is selectively activated by face stimuli~\cite{kanwisher2006fusiform, tsao2008mechanisms}. This biological specialization makes FIQA particularly sensitive to both stimulus features (e.g., age, gender, ethnicity, attractiveness) and the demographic background of the observers.

An even greater challenge arises when evaluating the quality of steganographically distorted facial images. Steganography is the practice of concealing information within digital media, typically by subtly modifying pixel values in a way that is imperceptible to human observers~\cite{steganography}. Although visually unobtrusive, these alterations can degrade biometric features and compromise recognition performance. NR-IQA approaches, while not requiring references, are generally not designed to detect such imperceptible, task-relevant distortions.

To address these limitations, we propose a hybrid FIQA framework based on pseudo-Mean Opinion Scores (pseudo-MOS). Beginning with a small subset of facial images labeled with subjective MOS, we train a regression-based fusion model that integrates multiple classical FR-IQA scores into a single, perceptually aligned quality measure. This FR fusion model is then applied to a larger dataset to generate pseudo-MOS labels for images lacking ground-truth annotations. A NR regression model is subsequently trained on deep features extracted from a ResNet-18 pretrained on ImageNet, enabling perceptual quality prediction without the need for reference images.

Our approach bridges the gap between FR supervision and NR inference. It provides a scalable solution for IQA in images subjected to complex, low-visibility distortions such as steganography, and lays the groundwork for adaptable, domain-specific quality assessment models.
