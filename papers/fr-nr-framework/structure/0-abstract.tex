\begin{abstract}

A persistent discrepancy exists between standard Image Quality Assessment (IQA) metrics and human perceptual judgments, typically quantified through Mean Opinion Scores (MOS).
This gap poses a critical challenge for tasks where visual quality directly impacts performance, such as facial recognition and visual data transmission.
In particular, assessing the perceptual quality of steganographically distorted facial images remains difficult, especially in the absence of reference images.
To address this, we introduce a hybrid Full-to-No-Reference framework for Face Image Quality Assessment (FIQA), built upon a learning strategy based on pseudo-MOS.\@
A full-reference fusion metric is first trained by regressing multiple classical IQA scores against human MOS on a subset of a facial dataset.
This metric is then applied to the full dataset to generate pseudo-MOS labels.
Using deep features extracted from a ResNet-18 model pretrained on ImageNet, we train a no-reference regressor capable of predicting perceptual quality.
The proposed framework bridges full-reference supervision and no-reference inference, offering a scalable and accurate solution for FIQA in challenging conditions and paving the way for application-specific, data-driven IQA designs.

\keywords{Face IQA \and Steganography \and No-Reference IQA \and Pseudo-MOS \and Full-Reference Fusion \and Deep Regression}
\end{abstract}
