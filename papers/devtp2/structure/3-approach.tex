\section{Approach}

We implemented and evaluated three classes of generative models. Each model was adapted to generate 28$\times$28 RGB images that resemble the blood cell samples in the BloodMNIST dataset.

For each approach, the model was trained on the entire dataset, combining the original training, validation, and test splits. All images were normalized to the $[-1, 1]$ range. We maintained a batch size of 128 and used the Adam optimizer for all models. Each training was repeated with five random seeds for later evaluation stability.

\textbf{VAE:} The Variational Autoencoder consists of a convolutional encoder that maps the input image into a latent Gaussian distribution, and a decoder that reconstructs the image from sampled latent vectors. The model is trained using a reconstruction loss combined with a Kullback-Leibler divergence term to regularize the latent space~\cite{kingma2013auto}.

\textbf{GAN:} We implemented a Deep Convolutional GAN (DCGAN), with a generator that learns to synthesize realistic images from latent noise vectors, and a discriminator that learns to distinguish between real and generated samples~\cite{goodfellow2014generative}. Both networks use strided convolutions and LeakyReLU activations. The generator is trained to fool the discriminator, while the discriminator is trained to correctly classify real and fake inputs.

\textbf{Diffusion:} The DDPM is a generative model based on a Markovian process that gradually adds noise to images and then learns to reverse this corruption step by step~\cite{ho2020denoising}. The training objective is to predict the added noise at each timestep, enabling the model to sample clean images from pure noise.

To ensure fairness, all models were trained using the same hardware environment and epochs, and evaluated using the Fr√©chet Inception Distance (FID) as a common metric.