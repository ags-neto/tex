\section{Experimental Setup}

All code and experiments are documented in the accompanying Jupyter Notebook, developed with Python 3.10.12 and PyTorch 2.1.0, running under CUDA 11.8 on a machine equipped with an NVIDIA GeForce GTX 970 GPU.\@ The BloodMNIST dataset was loaded via the MedMNIST v2 framework, and all images were resized to $28 \times 28$ with three color channels. To simplify the setup, the training, validation, and test splits were merged into a single training set of 17,092 images.

Each generative model was trained for 20 epochs using a batch size of 128. The Adam optimizer was employed with default parameters ($\beta_1=0.9$, $\beta_2=0.999$), and a learning rate of $10^{-3}$ for the VAE and Diffusion models, and $2\times10^{-4}$ for the GAN.\@

During evaluation, each model generated 10,000 synthetic images from latent noise vectors (or denoised noise in the case of the diffusion model). These samples were compared against 10,000 randomly selected real images using FID.\@ To ensure statistical robustness, this evaluation was repeated across five independent runs with different different seeds. The final score for each model is reported as the mean $\mu$ and standard deviation $\sigma$ of the FID values across these runs.

Let $\text{FID}_i$ denote the score obtained in run $i$, for $i = 1, \dots, 5$. The reported result is 
\begin{equation}
    \text{FID}_{\text{final}} = \mu \pm \sigma
\end{equation}
where
\begin{align}
    \mu &= \frac{1}{5} \sum_{i=1}^5 \text{FID}_i \\
    \sigma &= \sqrt{\frac{1}{5} \sum_{i=1}^5 {\left(\text{FID}_i - \mu\right)}^2}
\end{align}
