\chapter{Related Work}\label{chap:related_work}


\section{Subjective IQA}\label{sec:subjective_iqa}

Subjective image quality assessment is grounded in human visual perception and remains the gold standard for evaluating visual fidelity. Standardized by ITU-R BT.500~\cite{ITU-R-BT500} and ITU-T P.910~\cite{itut2006}, traditional methodologies rely on controlled laboratory conditions, calibrated displays, and predefined rating procedures to ensure reproducibility and validity.

\subsection{Assessment Protocols}\label{sec:assessment_protocols}

Assessment protocols are typically divided into two categories: quality assessment, which estimates the overall perceived quality of an image, and impairment assessment, which evaluates the severity of degradation relative to an undistorted reference. In image evaluation, two main methodological paradigms are used: Single Stimulus (SS), where one image is shown at a time and rated for quality; and Double Stimulus (DS), where both the reference and distorted images are shown together to assess perceptual difference. Rating scales can be numerical (e.g., 1 to 11, or 1 to 100) or categorical, with labels for quality (e.g., bad, poor, fair, good, excellent) or impairment (e.g., very annoying, annoying, slightly annoying, perceptible but not annoying, imperceptible). Each method differs in cognitive demand, sensitivity to bias, and statistical power, and the appropriate choice depends on the specific goals of the assessment.

After subjective scores are collected, typically from at least thirty observers per image, the data undergo rigorous statistical screening to ensure reliability and consistency. The first step involves identifying and discarding outlier observers whose scoring behavior significantly deviates from the population. This is usually performed by computing the Pearson correlation coefficient between an observer's scores and the preliminary mean scores across all images. Observers whose correlation falls below a predefined threshold (e.g., $r < 0.75$) are flagged as unreliable and removed. Following outlier removal, the scores are averaged to compute the final Mean Opinion Score (MOS) or Difference Mean Opinion Score (DMOS), depending on whether quality or impairment was assessed.

The MOS is computed as the arithmetic mean of all valid scores for a given image:

\begin{equation}
\text{MOS} = \frac{1}{N} \sum_{i=1}^{N} S_i
\end{equation}
where $S_i$ is the score given by the $i$-th subject, and $N$ is the number of valid observers.

For double-stimulus tests, the DMOS reflects the perceptual degradation with respect to a reference:

\begin{equation}
\text{DMOS} = \text{MOS}_{\text{reference}} - \text{MOS}_{\text{distorted}}
\end{equation}

Confidence intervals for MOS/DMOS are also computed, usually assuming a normal distribution. Statistical tests such as t-tests or ANOVA may be applied to assess differences between image groups or distortion types. This rigorous post-processing pipeline ensures that the final subjective scores are both perceptually meaningful and statistically valid.


\subsection{Crowdsourcing}\label{sec:crowdsourcing}

Beyond laboratory-controlled protocols, crowdsourcing has emerged as a scalable alternative for subjective IQA.\@ Platforms such as Amazon Mechanical Turk (AMT) and Prolific allow researchers to gather perceptual scores from a geographically and demographically diverse participant pool. To mitigate the loss of environmental control, studies often integrate reliability checks, such as repeated image pairs or gold-standard trials~\cite{mos2016}. While the quality of crowdsourced labels may vary, prior work has shown that when properly filtered, crowdsourced MOS can achieve correlation levels comparable to those from lab-based assessments~\cite{jin2020pipal}. This approach enables the creation of large-scale IQA datasets, significantly expanding the empirical foundation for training and benchmarking objective metrics.

\subsection{Benchmark Datasets}\label{sec:benchmark_datasets}

Several benchmark datasets have been curated to support the development and evaluation of IQA models. The LIVE~\cite{sheikh2006image}, TID2013~\cite{pnsr2003}, and CSIQ~\cite{ma2011psnr} datasets follow ITU-R BT.500-compliant procedures, offering high-quality MOS/DMOS annotations across a range of distortion types. More recent datasets, such as PIPAL~\cite{jin2020pipal} and KonIQ-10k~\cite{mos2016}, leverage crowdsourced assessments to achieve broader coverage of real-world content and perceptual variance. These datasets not only facilitate benchmarking but also allow for training data-driven IQA models in both full and no-reference scenarios. Nonetheless, domain-specific datasets, particularly those for biometric, medical, or forensic applications, remain scarce, limiting the generalizability of learned metrics in those contexts.


\section{Objective IQA}\label{sec:objective_iqa}

Objective IQA refers to the automatic estimation of visual quality using computational models. Unlike subjective methods that rely on human ratings, objective IQA provides consistent, scalable evaluations suitable for real-world tasks such as compression, transmission, restoration, and enhancement~\cite{gonzalez2002digital, sheikh2006image}. A wide range of objective metrics has been proposed, reflecting different assumptions about the human visual system and the nature of image distortions. Surveys such as Wang et al.~\cite{wang2004image}, Liu et al.~\cite{liu2013mmf}, and Ding et al.~\cite{ding2020dists} review dozens of existing metrics, highlighting both classical signal-based models and recent learning-based approaches. While no metric perfectly matches human perception, objective methods offer a low-cost, repeatable alternative for benchmarking, model optimization, and large-scale quality monitoring.

These metrics can be further characterized by the domain in which they operate and the spectral representation they use. The domain refers to the signal space employed during computation. Classical metrics operate in the spatial domain, comparing pixel intensities directly, as seen in PSNR and SSIM~\cite{Wang2004SSIM}. Others use the frequency domain, employing transforms such as the discrete cosine transform (DCT) or wavelet decompositions to capture structural differences, as in VIF~\cite{sheikh_vif_2006} and IFC~\cite{sheikh2005ifc}. A third group targets the spectral domain, often used in remote sensing, where images are compared across multiple wavelength bands using metrics such as ERGAS~\cite{Ranchin2000ERGAS} and SAM~\cite{Kruse1993SAM}. Additional domains include gradient-based methods that emphasize edge strength or direction, such as GMSD~\cite{Xue2014GMSD}, and deep feature domains, which assess perceptual similarity using activations from convolutional neural networks trained on natural images, as in LPIPS~\cite{zhang2018lpips} and DISTS~\cite{ding2020dists}.

The spectral representation, or color spectrum, denotes the channel configuration used by the metric. Many classical approaches focus solely on the luminance channel (Y), which aligns closely with human perception~\cite{sheikh_vif_2006, chandler2007vsnr}. Others compute scores over grayscale or full RGB images, as seen in FSIM~\cite{zhang2011fsim} and its color extension FSIMc. Some metrics operate in alternative color spaces such as YCbCr or CIELab~\cite{Wang2002PQM}, while hyperspectral or multiband metrics process dozens of spectral channels, particularly in remote sensing applications~\cite{Zhou1998SCC,Alparone2008QNR}. Table~\ref{tab:fr_iqa_metrics} summarizes the 40 IQA metrics considered in this work, detailing their reference type, computational domain, publication year, number of tested databases, and spectral characteristics.



\begin{longtable}{l l l c c l}
    \caption{Summary of the 41 IQA metrics used}\label{tab:fr_iqa_metrics}\\
    \hline % chktex 44
    \textbf{Metric} & \textbf{Type} & \textbf{Domain} & \textbf{Year} & \textbf{Tested Databases} & \textbf{Spectrum}\\
    \hline % chktex 44
    \endfirsthead % chktex 1
    \multicolumn{6}{r}{\small\itshape(continuation)}\\ % chktex 6
    \hline % chktex 44
    \textbf{Metric} & \textbf{Type} & \textbf{Domain} & \textbf{Year} & \textbf{Tested Databases} & \textbf{Spectrum}\\
    \hline % chktex 44
    \endhead % chktex 1
    \hline % chktex 44
    \endfoot % chktex 1
    
    C-SSIM~\cite{Hassan2017CSSIM}          & FR   & spatial             & 2017 & 2    & RGB \\
    CW-SSIM~\cite{Sampat2009CWSSIM}        & FR   & complex wavelet     & 2009 & 1    & grayscale \\
    DISTS~\cite{Ding2020DISTS}             & FR   & deep features       & 2020 & 4    & RGB \\
    DSSIM~\cite{Wang2004SSIM}              & FR   & spatial             & 2004 & ---  & grayscale \\
    ERGAS~\cite{Ranchin2000ERGAS}          & FR   & spectral            & 2000 & 1    & multi-band \\
    ESIM (ESSIM)~\cite{Zhang2013ESSIM}     & FR   & spatial (edges)     & 2013 & 2    & grayscale \\
    FIQ (IFC)~\cite{Sheikh2005IFC}         & FR   & wavelet             & 2005 & 1    & Y (luma) \\
    FSIM~\cite{Zhang2011FSIM}              & FR   & spatial             & 2011 & 3    & grayscale \\
    FSIMc~\cite{Zhang2011FSIM}             & FR   & spatial             & 2011 & 3    & RGB \\
    G-SSIM~\cite{Liu2012GSSIM}             & FR   & spatial (gradients) & 2012 & 2    & grayscale \\
    GMS + CMS~\cite{Zhang2011FSIM}         & FR   & spatial             & 2011 & 3    & RGB \\
    GMSD~\cite{Xue2014GMSD}                & FR   & spatial (gradients) & 2014 & 3    & grayscale \\
    IW-SSIM~\cite{Li2011IWSSIM}            & FR   & spatial             & 2011 & 4    & grayscale \\
    L-SSIM~\cite{Wang2004SSIM}             & FR   & spatial             & 2004 & ---  & grayscale \\
    LPIPS-Alex~\cite{Zhang2018LPIPS}       & FR   & deep features       & 2018 & 4    & RGB \\
    LPIPS-Squeeze~\cite{Zhang2018LPIPS}    & FR   & deep features       & 2018 & 4    & RGB \\
    LPIPS-VGG~\cite{Zhang2018LPIPS}        & FR   & deep features       & 2018 & 4    & RGB \\
    MAE~\cite{Gonzalez2008DIP}             & FR   & spatial             & N/A  & ---  & grayscale \\
    MS-FSIM~\cite{Zhang2011FSIM}           & FR   & spatial             & 2011 & 3    & grayscale \\
    MS-SSIM~\cite{Wang2003MSSSIM}          & FR   & spatial             & 2003 & 1    & grayscale \\
    MSE~\cite{Gonzalez2008DIP}             & FR   & spatial             & N/A  & ---  & grayscale \\
    PQM~\cite{Wang2002PQM}                 & NR   & spatial             & 2002 & 1    & Y (JPEG) \\
    PSNR~\cite{Gonzalez2008DIP}            & FR   & spatial             & N/A  & ---  & grayscale \\
    PSNR-B~\cite{Yim2011PSNRB}             & FR   & spatial             & 2011 & 1    & Y (luma) \\
    R-SSIM~\cite{Malpica2008RSSIM}         & FR   & spatial (depth)     & 2009 & 1    & depth map \\
    RMSE~\cite{Gonzalez2008DIP}            & FR   & spatial             & N/A  & ---  & grayscale \\
    SAM~\cite{Kruse1993SAM}                & FR   & spectral            & 1993 & 1    & multi-band \\
    SCC~\cite{Zhou1998SCC}                 & FR   & spectral/spatial    & 1998 & 1    & multi-band \\
    SNR~\cite{Gonzalez2008DIP}             & FR   & spatial             & N/A  & ---  & grayscale \\
    SPIQ~\cite{Chen2022SPIQ}               & NR   & deep features       & 2022 & 3    & RGB \\
    SR-SIM~\cite{Zhang2012SRSIM}           & FR   & spatial             & 2012 & 3    & RGB \\
    SSIM~\cite{Wang2004SSIM}               & FR   & spatial             & 2004 & 1    & grayscale \\
    UQI~\cite{Wang2002UQI}                 & FR   & spatial             & 2002 & 1    & grayscale \\
    VIF~\cite{Sheikh2006VIF}               & FR   & wavelet             & 2006 & 1    & Y (luma) \\
    VIFc~\cite{Sheikh2006VIF}              & FR   & wavelet             & 2006 & 1    & Y/CbCr \\
    VIFp~\cite{Sheikh2005VIFp}             & FR   & pixel-domain        & 2005 & 1    & Y (luma) \\
    VSNR~\cite{chandler2007vsnr}           & FR   & wavelet             & 2007 & 1    & Y (luma) \\
    WaDIQaM~\cite{Bosse2018WaDIQaM}        & FR   & deep features       & 2018 & 3    & RGB \\
    W-SSIM~\cite{Engelke2011WSSIM}         & FR   & spatial             & 2011 & 1    & grayscale \\
    $D_{\lambda}$ (QNR)~\cite{Alparone2008QNR} & FR & spectral/spatial  & 2008 & 1    & multi-band \\
\end{longtable}

\subsection{Full-Reference IQA}\label{sec:full_reference_iqa}

FR-IQA models estimate the perceptual quality of a distorted image by comparing it to an undistorted reference. These methods assume complete access to the original image and are commonly used in contexts where ground-truth data is available, such as image compression, transmission, enhancement, and restoration~\cite{sheikh2006image, wang2004image}. By directly quantifying deviations from the reference, FR-IQA provides a consistent and interpretable benchmark for evaluating distortion. However, their applicability is limited to scenarios where high-quality reference images exist, making them unsuitable for many real-world applications~\cite{mittal2012making, hosu2020koniq}.


\subsubsection{Fidelity-Based Metrics}\label{sec:error_based_metrics}

Fidelity-based metrics quantify distortion by measuring direct numerical differences between the reference and the distorted image. These models operate in the spatial domain and are widely used due to their simplicity, computational efficiency, and clear interpretability~\cite{gonzalez2002digital, wang2009mean}. The most common examples include the mean squared error (MSE), root mean squared error (RMSE), and peak signal-to-noise ratio (PSNR). Although these metrics are sensitive to pixel-level changes, they often fail to capture perceptually relevant distortions, especially when the structure or semantic content of the image is preserved~\cite{wang2004image, chandler2007vsnr}. Extensions such as PSNR-B address some of these limitations by incorporating blocking artifact penalties~\cite{yim2011psnrb}, while other domain-specific metrics, such as ERGAS~\cite{ranchin2000ergas} and SAM~\cite{kruse1993sam}, are used in remote sensing to assess multiband and hyperspectral image fidelity.

The Mean Squared Error (MSE) measures the average squared pixel-wise difference:

\begin{equation}
\text{MSE} = \frac{1}{N} \sum_{i=1}^{N} {x_i - y_i}^2
\end{equation}

The Root Mean Squared Error (RMSE) is the square root of MSE:\@

\begin{equation}
\text{RMSE} = \sqrt{\text{MSE}}
\end{equation}

The Mean Absolute Error (MAE) computes the average of absolute differences:

\begin{equation}
\text{MAE} = \frac{1}{N} \sum_{i=1}^{N} |x_i - y_i|
\end{equation}

The Peak Signal-to-Noise Ratio (PSNR) expresses the logarithmic ratio between the maximum possible signal value and the MSE between a reference and a distorted image:

\begin{equation}
\text{PSNR} = 10 \cdot \log_{10} \left( \frac{MAX^2}{\text{MSE}} \right)
\end{equation}
where $MAX$ is the maximum possible pixel value (typically 255 for 8-bit images). Although widely used for its simplicity and interpretability, PSNR correlates poorly with human perception, especially in the presence of structured distortions. 

To address this limitation, PSNR-B extends the original formulation by incorporating a blocking effect factor (BEF), which penalizes compression artifacts such as those introduced by block-based codecs:

\begin{equation}
\text{PSNR-B} = 10 \cdot \log_{10} \left( \frac{MAX^2}{\text{MSE} + \text{BEF}} \right)
\end{equation}

The Signal-to-Noise Ratio (SNR) compares the signal power to the noise power:

\begin{equation}
\text{SNR} = 10 \cdot \log_{10} \left( \frac{\sum x_i^2}{\sum {(x_i - y_i)}^2} \right)
\end{equation}

The Spectral Angle Mapper (SAM), often used in hyperspectral imagery, computes the angle between image vectors:

\begin{equation}
\text{SAM} = \cos^{-1} \left( \frac{\langle x, y \rangle}{\|x\| \cdot \|y\|} \right)
\end{equation}

The ERGAS (Erreur Relative Globale Adimensionnelle de Synthèse) metric expresses relative global error and is frequently used in image fusion and remote sensing:

\begin{equation}
\text{ERGAS} = 100 \cdot \frac{h}{l} \cdot \sqrt{ \frac{1}{N} \sum_{i=1}^{N} {\left( \frac{\text{RMSE}_i}{\mu_i} \right)}^2 }
\end{equation}
where $h$ and $l$ are the spatial resolutions of the high and low resolution images respectively, $\text{RMSE}_i$ is the RMSE of band $i$, and $\mu_i$ is the mean of the reference band.

These metrics are particularly useful when high pixel fidelity is essential, but they often fail to align with human perception, especially in cases where structural or semantic information is preserved.


\subsubsection{Perceptual Similarity Metrics}\label{sec:perceptual_metrics}

Perceptual similarity metrics aim to model the characteristics of the human visual system (HVS), focusing not on raw signal differences but on how distortions affect perceived quality. These models evaluate structural, luminance, and contrast relationships within local regions of the image and are typically more aligned with subjective scores than purely fidelity-based metrics~\cite{wang2004image,chandler2007vsnr}. The Structural Similarity Index (SSIM) is a foundational method in this category, comparing local windows between the reference and distorted images. Multiscale variants such as MS-SSIM~\cite{Wang2003MSSSIM} improve robustness across spatial resolutions. Other models, such as FSIM~\cite{Zhang2011FSIM}, incorporate phase congruency and gradient information to better reflect visual saliency, while GMSD~\cite{Xue2014GMSD} measures image quality by analyzing the standard deviation of gradient magnitude similarity. Additional extensions include SR-SIM~\cite{Zhang2012SRSIM}, which prioritizes salient regions, and W-SSIM~\cite{Engelke2011WSSIM}, which introduces perceptual weighting across scales. These methods offer improved correlation with human opinion scores, particularly in the presence of structural or perceptual distortions.

The Structural Similarity Index (SSIM) compares local image patches between the reference and distorted images, measuring similarity in luminance, contrast, and structure. It is defined as:

\begin{equation}
\text{SSIM}(x, y) = \frac{(2\mu_x \mu_y + C_1)(2\sigma_{xy} + C_2)}{(\mu_x^2 + \mu_y^2 + C_1)(\sigma_x^2 + \sigma_y^2 + C_2)}
\end{equation}
where $\mu_x$ and $\mu_y$ are the local means, $\sigma_x^2$ and $\sigma_y^2$ are the local variances, $\sigma_{xy}$ is the local covariance, and $C_1$, $C_2$ are stabilizing constants.

Multiscale SSIM (MS-SSIM) extends SSIM by computing similarity at multiple image scales. It is defined as:

\begin{equation}
\text{MS-SSIM}(x, y) = \prod_{j=1}^{M} {[l_j(x, y)]}^{\alpha_j} \cdot {[c_j(x, y)]}^{\beta_j} \cdot {[s_j(x, y)]}^{\gamma_j}
\end{equation}

where $l_j$, $c_j$, and $s_j$ are the luminance, contrast, and structure comparisons at scale $j$, and $\alpha_j$, $\beta_j$, $\gamma_j$ are weights.

The Feature Similarity Index (FSIM) combines phase congruency (PC) and gradient magnitude (GM) to evaluate perceptual quality:

\begin{equation}
\text{FSIM}(x, y) = \frac{\sum_{i \in \Omega} T(i) \cdot PC_m(i)}{\sum_{i \in \Omega} PC_m(i)}
\end{equation}
where $T(i)$ is a similarity function combining gradient and phase congruency similarity at pixel $i$, and $PC_m(i)$ is the maximum phase congruency across both images.

The Gradient Magnitude Similarity Deviation (GMSD) is based on the standard deviation of pixel-wise gradient similarity maps:

\begin{equation}
\text{GMS}(i) = \frac{2G_x(i) G_y(i) + C}{{G_x(i)}^2 + {G_y(i)}^2 + C}
\end{equation}

\begin{equation}
\text{GMSD}(x, y) = \sqrt{\frac{1}{N} \sum_{i=1}^{N} {\left( \text{GMS}(i) - \overline{\text{GMS}} \right)}^2}
\end{equation}
where $G_x(i)$ and $G_y(i)$ are gradient magnitudes of the reference and distorted images, and $C$ is a small constant.

The Spectral Residual Similarity Index (SR-SIM) evaluates similarity by combining spectral residual saliency and gradient magnitude:

\begin{equation}
\text{SR-SIM}(x, y) = \frac{\sum_{i \in \Omega} S_r(i) \cdot G_m(i) \cdot Q(i)}{\sum_{i \in \Omega} S_r(i) \cdot G_m(i)}
\end{equation}

where $S_r(i)$ is the spectral residual saliency map, $G_m(i)$ is the gradient magnitude, and $Q(i)$ is the local structural similarity.

The Information-Weighted Structural Similarity Index (IW-SSIM) assigns greater importance to image regions that carry more structural information. The final score is a weighted average of SSIM values computed at each local window:

\begin{equation}
\text{IW-SSIM}(x, y) = \frac{\sum_{i=1}^{N} w_i \cdot \text{SSIM}_i(x, y)}{\sum_{i=1}^{N} w_i}
\end{equation}
where $w_i$ is the information weight for window $i$, often computed using local entropy or local variance.

The Wavelet-based SSIM (W-SSIM) extends SSIM into the complex wavelet domain. Instead of operating directly on image intensities, it evaluates phase consistency across wavelet subbands. The general form follows a similar structure to SSIM, applied to wavelet coefficients:

\begin{equation}
\text{W-SSIM}(x, y) = \prod_{s} \prod_{o} \text{SSIM}(x_{s,o}, y_{s,o})
\end{equation}

where $s$ indexes the scale and $o$ the orientation in the wavelet decomposition, and $x_{s,o}$ and $y_{s,o}$ are the corresponding coefficients.

The Edge Strength Similarity Metric (ESSIM or ESIM) modifies SSIM by replacing the structure component with a comparison of edge magnitudes. The edge strength is typically extracted via a Sobel or Prewitt operator. The structure term $\sigma_{xy}$ is replaced by a directional edge similarity $E(x, y)$, such that:

\begin{equation}
\text{ESSIM}(x, y) = \frac{(2\mu_x \mu_y + C_1)(2E(x, y) + C_2)}{(\mu_x^2 + \mu_y^2 + C_1)(E(x, x) + E(y, y) + C_2)}
\end{equation}
where $E(x, y)$ denotes the dot product of edge maps from the two images.

While perceptual similarity metrics approximate human visual perception by analyzing structural and saliency-based cues, they remain largely deterministic and handcrafted.

\subsubsection{Information-theoretic Metrics}\label{sec:learning_based_metrics}

Information-theoretic approaches offer a fundamentally different perspective, treating image quality assessment as a problem of quantifying the amount of perceptually relevant information preserved in the distorted image. These models are rooted in natural scene statistics and signal fidelity theory, where both the reference and distorted images are interpreted as stochastic signals~\cite{sheikh2005ifc}. The Information Fidelity Criterion (IFC) and the Visual Information Fidelity (VIF) index model the image source and distortion process probabilistically, estimating mutual information between the reference and distorted signals as mediated by models of the human visual system~\cite{Sheikh2006VIF}. This framework enables a content-aware assessment of quality that accounts for the statistical dependencies and perceptual importance of image structures.

The IFC models image patches in the wavelet domain using Gaussian scale mixture (GSM) models. It estimates the mutual information between the reference image $x$ and the distorted image $y$ conditioned on the distortion model and natural scene statistics:

\begin{equation}
\text{IFC}(x, y) = \sum_{i \in \Omega} I(C_{x,i}; C_{y,i} \mid z_i)
\end{equation}
where $C_{x,i}$ and $C_{y,i}$ are wavelet coefficients of the reference and distorted images in patch $i$, and $z_i$ is a local variance parameter estimated under a GSM model. The total information is accumulated over all image patches $\Omega$.

The VIF index extends this by incorporating a model of the human visual system. It computes the ratio of information that can be extracted by a hypothetical observer from the distorted image versus the reference:

\begin{equation}
\text{VIF}(x, y) = \frac{\sum_{i=1}^{N} I(E_i; F_i)}{\sum_{i=1}^{N} I(E_i; G_i)}
\end{equation}

Here, $E_i$ represents the coefficients of the reference image, $F_i$ those of the distorted image after HVS filtering, and $G_i$ those of the reference after the same filtering. The numerator measures the information preserved in the distorted image, and the denominator the total information in the reference. Mutual information is computed under assumed Gaussian models of natural scenes and additive noise for distortion and perceptual masking.

Several variants of the VIF metric have been proposed to adapt the model to different domains and signal characteristics. The pixel-domain variant, VIFp~\cite{Sheikh2005VIFp}, simplifies the original VIF model by operating directly on pixel intensities rather than wavelet coefficients. This avoids the need for multi-resolution decomposition and is computationally more efficient, making it suitable for real-time or resource-constrained applications. VIFp retains the same information fidelity framework, estimating mutual information between corresponding local patches.

Another extension is VIFc, which extends the VIF model to handle color images~\cite{Sheikh2006VIF}. Instead of converting the image to grayscale, VIFc applies the information fidelity model to each color channel, often in the YCbCr color space. A weighted fusion of channel-wise VIF scores is then used to produce a final quality estimate:

\begin{equation}
\text{VIFc}(x, y) = \sum_{k \in \{Y,Cb,Cr\}} w_k \cdot \text{VIF}_k(x, y)
\end{equation}
where $w_k$ are empirically determined weights that reflect the perceptual importance of each component. Typically, the luminance channel (Y) receives the highest weight due to its dominant role in human perception.

These variants preserve the conceptual integrity of the original VIF model while offering improved flexibility for different input modalities and computational budgets.

\subsubsection{Deep Learning-based Metrics}\label{sec:deep_learning_based_metrics}

Learning-based full-reference IQA models leverage deep neural networks to capture perceptual similarity in a data-driven manner. Unlike traditional models that rely on handcrafted features or analytic formulations of human perception, these methods operate in learned feature spaces derived from large-scale image datasets. The central assumption is that perceptual similarity can be approximated by comparing intermediate activations of pretrained convolutional neural networks (CNNs)~\cite{Zhang2018LPIPS}. The Learned Perceptual Image Patch Similarity (LPIPS) metric exemplifies this approach by computing a weighted $L_2$ distance between feature maps extracted from networks such as VGG or AlexNet. Subsequent models, such as DISTS~\cite{ding2020dists}, refine this idea by balancing structural and texture similarity through adaptive weighting schemes. Other approaches, like WaDIQaM~\cite{Bosse2018WaDIQaM}, train task-specific regressors over deep features extracted from both the reference and distorted images. These models achieve high correlation with human opinion scores, particularly in cases involving complex or perceptually subtle distortions, but often require careful normalization and calibration of feature space distances.

The LPIPS metric computes the distance between deep feature maps of a reference image $x$ and a distorted image $y$ as follows:

\begin{equation}
\text{LPIPS}(x, y) = \sum_{l} \frac{1}{H_l W_l} \sum_{h=1}^{H_l} \sum_{w=1}^{W_l} \| w_l \odot (\hat{f}_l^x(h, w) - \hat{f}_l^y(h, w)) \|_2^2
\end{equation}
where $\hat{f}_l^x$ and $\hat{f}_l^y$ are the normalized feature maps at layer $l$ for images $x$ and $y$, respectively, with spatial dimensions $H_l \times W_l$. The weights $w_l$ are learned scalars for each channel, and $\odot$ denotes element-wise multiplication.

The DISTS metric combines feature similarity and texture similarity between corresponding feature maps:

\begin{equation}
\text{DISTS}(x, y) = \sum_{l} \alpha_l \cdot S_l(x, y) + \beta_l \cdot T_l(x, y)
\end{equation}
where $S_l(x, y)$ is the structure similarity component computed as the cosine similarity between deep features at layer $l$, and $T_l(x, y)$ is the texture similarity component, often based on the mean and variance of feature activations. The coefficients $\alpha_l$ and $\beta_l$ are learned to balance structure and texture contributions for each layer $l$.

WaDIQaM-FR learns a patch-based quality prediction model using paired deep features from the reference image $x$ and the distorted image $y$. Given a patch pair $(x_i, y_i)$, deep features are extracted using a shared-weight CNN encoder, and the resulting representations are concatenated and passed through a regression network to predict a local quality score $q_i$. The final quality score is the weighted average over all patches:

\begin{equation}
Q(x, y) = \sum_{i=1}^{N} w_i \cdot q_i
\end{equation}
where $q_i = f_{\theta}(x_i, y_i)$ is the predicted quality for patch $i$, $w_i$ is a learned spatial weight indicating the perceptual relevance of patch $i$, and $N$ is the total number of patches. The function $f_{\theta}$ denotes the trained regression model with parameters $\theta$.

During training, the network minimizes the error between predicted quality scores and human-annotated MOS or DMOS labels using an appropriate loss function (e.g., MSE or Huber loss). Unlike LPIPS or DISTS, WaDIQaM does not rely on pretrained networks, allowing it to adapt feature representations to the task of quality prediction.

In summary, full-reference IQA models span a spectrum from simple error-based metrics to perceptually and statistically grounded approaches. Fidelity-based methods offer computational efficiency but lack alignment with human perception. Perceptual similarity models incorporate structural and saliency cues to improve correlation with subjective quality. Information-theoretic metrics formalize quality as mutual information preservation under natural scene statistics. Learning-based models leverage deep features or end-to-end training to approximate perceptual similarity in high-dimensional spaces. Each class reflects a trade-off between interpretability, perceptual fidelity, and computational complexity, making their suitability highly context-dependent.

\subsection{No-Reference IQA}\label{sec:no_reference_iqa}

\subsubsection{Traditional NR-IQA}\label{sec:traditional_nr_iqa}

Among traditional NR-IQA techniques, PIQE~\cite{piqe2016} (Perception-based Image Quality Evaluator) and NIQE~\cite{mittal2013making} (Natural Image Quality Evaluator) are widely used due to their computational simplicity. PIQE begins by dividing the input image into non-overlapping $16 \times 16$ blocks, denoted $B_i$, and identifying distorted blocks based on edge content and local variance thresholds. For each block, it computes perceptual features using the gradient magnitude:
\begin{equation}
S_i = \frac{1}{|B_i|} \sum_{(x,y) \in B_i} \sqrt{ {\left( \frac{\partial I}{\partial x} \right)}^2 + {\left( \frac{\partial I}{\partial y} \right)}^2 },
\end{equation}
and noise, estimated by subtracting a low-pass filtered (LPF) version of the block:
\begin{equation}
N_i = \text{std}\left( B_i - \text{LPF}(B_i) \right).
\end{equation}
The global PIQE score is computed via a weighted aggregation over the distorted blocks, $\mathcal{D}$:
\begin{equation}
\text{PIQE} = \frac{1}{|\mathcal{D}|} \sum_{i \in \mathcal{D}} w_i \cdot ( \alpha S_i + \beta N_i + \gamma C_i ),
\end{equation}
where $w_i$ are confidence weights and $\alpha, \beta, \gamma$ are scaling factors for sharpness, noise, and contrast, respectively. The final score is scaled between 0 (best quality) and 100 (worst quality).

In contrast, NIQE is grounded in natural scene statistics (NSS), using a statistical deviation model to quantify perceptual degradation. First, it applies Mean Subtracted Contrast Normalization (MSCN) to locally standardize luminance:
\begin{equation}
\hat{I}(x, y) = \frac{I(x, y) - \mu(x, y)}{\sigma(x, y) + \epsilon},
\end{equation}
from which it extracts statistical features associated with Generalized Gaussian Distributions (GGD) and Asymmetric GGD (AGGD). A multivariate Gaussian (MVG) model with mean $\mu_r$ and covariance $\Sigma_r$ is estimated to a set of pristine images. During inference, the Mahalanobis distance between the test image features $\mathbf{f}_t$ and the natural image model is used to compute the NIQE score:
\begin{equation}
\text{NIQE}(\mathbf{f}_t) = \sqrt{ {(\mathbf{f}_t - \mu_r)}^T \Sigma_r^{-1} (\mathbf{f}_t - \mu_r) }.
\end{equation}
This formulation enables NIQE to operate in a completely opinion-unaware and unsupervised fashion, relying solely on deviations from natural image statistics to infer perceptual quality.

\subsubsection{Facial Image Quality Assessment}\label{sec:fiqa}

FIQA refers to the estimation of biometric utility from face images, typically in the context of face recognition, verification, or detection pipelines. Unlike generic NR-IQA, FIQA does not aim to measure aesthetic or perceptual quality, but rather the suitability of a face image for recognition purposes~\cite{damer2021localfusion, best2018faceqnet}. This quality is influenced by a range of factors, including resolution, pose, occlusion, blur, compression, and illumination.

Traditional FIQA methods relied on handcrafted features such as sharpness, symmetry, or inter-eye distance~\cite{grother2003facequality}. Recent models leverage deep face embeddings extracted from recognition networks~\cite{terhorst2020serfiq, terhorst2022quality}. These models typically apply a regression or ranking loss to map features to quality scores correlated with recognition accuracy.

FIQA is an inherently task-dependent quality problem. Unlike general-purpose IQA, it must account for the characteristics of the recognition model and dataset. Furthermore, FIQA methods often correlate poorly with perceptual quality metrics, as an image may appear visually high-quality but remain unsuitable for matching due to pose or occlusion. This motivates the need to explore perceptually grounded IQA models tailored to face data, and to investigate how traditional metrics can be adapted, fused, or compared against biometric-specific quality indicators.


In contrast to traditional statistical NR-IQA methods, recent approaches such as SER-FIQ~\cite{terhorst2020serfiq} (Stochastic Embedding Robustness for Face Image Quality) and MagFace~\cite{meng2021magface} adopt deep learning-based strategies tailored specifically for facial image quality. SER-FIQ builds on the intuition that high-quality facial images produce consistent embeddings under stochastic dropout perturbations, while low-quality images result in more variable and unstable representations. Given a face image, a pre-trained face recognition model (e.g., ArcFace) with dropout enabled generates $T$ stochastic embeddings $\{\mathbf{e}_1, \mathbf{e}_2, \dots, \mathbf{e}_T\} \in \mathbb{R}^d$. The pairwise cosine similarities between these embeddings are then computed:
\begin{equation}
\text{sim}(\mathbf{e}_i, \mathbf{e}_j) = \frac{\mathbf{e}_i \cdot \mathbf{e}_j}{\|\mathbf{e}_i\| \|\mathbf{e}_j\|}, \quad \forall i,j \in \{1,\dots,T\}, i < j.
\end{equation}
The final SER-FIQ score is derived from the mean or standard deviation of the similarity matrix, often defined as:
\begin{equation}
\text{SER-FIQ} = \frac{1}{\binom{T}{2}} \sum_{i<j} \text{sim}(\mathbf{e}_i, \mathbf{e}_j),
\end{equation}
where higher scores indicate more stable (and thus higher-quality) facial representations. This method is fully unsupervised and exploits intrinsic model uncertainty as an indicator for quality.

MagFace, on the other hand, is a supervised approach that explicitly learns a quality-aware embedding space. It introduces a margin-adaptive loss that couples facial feature magnitude with image quality. For an embedding \( \mathbf{e} \in \mathbb{R}^d \), its \( L_2 \)-norm \( \|\mathbf{e}\| \) is used as a measure for quality. The classification loss is modified as:
\begin{equation}
\mathcal{L}_{\text{MagFace}} = -\log \frac{e^{s \cdot (\cos(\theta_y + m(\|\mathbf{e}\|)))}}{e^{s \cdot (\cos(\theta_y + m(\|\mathbf{e}\|)))} + \sum_{j \neq y} e^{s \cdot \cos(\theta_j)}},
\end{equation}
where \( \theta_y \) is the angle between \( \mathbf{e} \) and the weight vector of the correct class, \( s \) is a scaling factor, and \( m(\|\mathbf{e}\|) \) is a dynamic margin that increases with quality. The final MagFace quality score is proportional to the magnitude \( \|\mathbf{e}\| \), constrained during training to lie within a fixed interval \([l, u]\) for regularization. Unlike SER-FIQ, MagFace learns to embed quality-awareness directly into the feature space through supervision, enabling both verification and quality prediction in a unified framework.


\subsection{Fusion-Based IQA Models}\label{sec:fusion_iqa}

To address the perceptual limitations of individual image quality metrics, several fusion-based approaches have been proposed. Liu et al.~\cite{liu2013mmf} introduced a multi-method fusion (MMF) framework that linearly combines full-reference IQA scores via supervised regression to better approximate MOS.\@ Henniger et al.~\cite{henniger2020biosig} developed a Random Forest model trained on handcrafted features derived from ISO-compliant facial images, improving performance in biometric settings. These works demonstrated that integrating complementary quality cues can significantly enhance perceptual alignment, outperforming standalone metrics in correlation with human judgment~\cite{robinson2020bias}.

In parallel, the scarcity of large-scale human-labeled datasets has motivated weakly supervised learning strategies. Chen et al.~\cite{chen2021pseudo} generated pseudo-MOS labels by averaging outputs of multiple FR-IQA metrics. RankIQA~\cite{liu2017rankiqa} applied relative ranking supervision on synthetically degraded images to learn ordinal quality representations. Wu et al.~\cite{wu2020cascaded} used cascaded convolutional regressors trained on pseudo-MOS labels, demonstrating that pseudo-supervision can bootstrap learning when subjective ground truth is limited.

These developments support the feasibility of learning perceptual quality through fusion, either via model ensembles or by generating training supervision. Our approach builds on this principle: we construct a regression-based fusion model that aggregates multiple FR-IQA scores to predict perceptual quality, using human-labeled MOS as supervision. This model is then applied to a larger unlabeled dataset to generate pseudo-MOS scores, which in turn are used to train a no-reference regressor. This hybrid full-to-no-reference framework enables scalable, perceptually grounded quality estimation even in the absence of pristine reference images.


\section{Steganography}\label{sec:steganography}

Steganography has evolved significantly with the rise of digital media and advanced compression algorithms, leading to a variety of embedding methods and detection strategies. Classical steganographic methods can be broadly categorized into spatial, frequency, and adaptive domain techniques. Spatial domain methods directly manipulate pixel values, most notably through Least Significant Bit (LSB) substitution~\cite{steganography2017}, to encode binary data within an image. Frequency domain methods operate on transformed representations of the image, using tools such as the Discrete Cosine Transform (DCT) or Discrete Wavelet Transform (DWT), embedding data in coefficients less sensitive to compression~\cite{steganography2016survey}. Adaptive methods further refine this by analyzing local characteristics of the image, dynamically choosing embedding regions to minimize perceptual distortion~\cite{steganography2018adaptive}.

However current methods cover a wide range of applications, there has been a growing interest in printer-proof steganography, which survives the printing and scanning process. This is particularly relevant in contexts where physical copies of images are distributed, such as MRTDs. The challenge lies in ensuring that the embedded information remains detectable and robust against distortions introduced by printing and scanning processes.

\subsection{Printer-proof Steganography}
Recent advances employ deep learning to design robust, end-to-end steganographic pipelines that simulate print-scan degradations during training. We highlight four such methods that represent the current state of the art in printer-proof steganography for natural and facial images.

\subsubsection{StegaStamp}
StegaStamp~\cite{stegastamp2020} introduced the first deep learning-based pipeline for robust steganography under real-world distortions. It jointly trains an encoder-decoder architecture with simulated perturbations, such as blur, color shift, projective warps, and JPEG compression, to mimic the print-scan process. A random bitstring is embedded into the image, and the decoder learns to retrieve it despite these degradations. The training loss combines a cross-entropy bit loss, an $L_2$ reconstruction loss, and a perceptual LPIPS loss to preserve visual quality. StegaStamp showed that it is possible to reliably recover short messages (e.g., 56 bits) from physically printed and scanned images, setting a benchmark for printer-proof robustness.

\subsubsection{CodeFace}
CodeFace~\cite{codeface2021} extends StegaStamp with a focus on facial images used in ID documents. It introduces facial-specific modules such as a Spatial Transformer Network (STN) for geometric alignment and a perceptual identity-preserving loss based on FaceNet embeddings. This ensures that the stego face remains visually and biometrically consistent with the original. The network is trained adversarially, with a discriminator encouraging naturalness and a decoder optimizing message recovery under print-scan distortions. CodeFace proves effective for embedding messages in high-resolution facial portraits, making it suitable for privacy-preserving biometric IDs.

\subsubsection{RiemStega}
RiemStega~\cite{cruz2025riemstega} innovates by introducing a Riemannian manifold-based loss. Instead of comparing pixel values, it represents images through symmetric positive definite (SPD) covariance matrices and minimizes the affine-invariant Riemannian distance between the cover and stego image descriptors. This statistical approach preserves global structural and texture patterns more robustly under degradation. The method maintains high visual fidelity while increasing message recoverability post-printing, and demonstrates strong performance on both generic and facial image datasets.

\subsubsection{StampOne}
StampOne~\cite{stampone2024} incorporates frequency-domain awareness by applying DWT and gradient maps to both image and message during encoding. The model balances frequency bands via a learned attention mechanism and includes frequency-domain discriminators in its loss functions. This results in encoded images with spectral properties closely matching those of the original, enhancing resilience to printer-related distortions such as color shifts and sensor noise. StampOne outperforms prior methods in both visual similarity and decoding accuracy, especially for high-frequency facial features.
