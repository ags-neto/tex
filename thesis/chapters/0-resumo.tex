Mantém-se uma discrepância persistente entre as métricas padrão de Avaliação da Qualidade de Imagem (IQA) e os juízos perceptivos humanos, geralmente quantificados através dos Mean Opinion Scores (MOS). Esta divergência constitui um desafio central em aplicações onde a qualidade percebida afeta o desempenho, como no reconhecimento facial. Apresentamos uma nova métrica de Avaliação da Qualidade de Imagens Faciais (FIQA) sem referência, desenvolvida no âmbito de uma estrutura de aprendizagem do tipo Full-to-No-Reference. O processo tem início com um modelo de fusão com referência completa, treinado para fazer a regressão de métricas IQA clássicas em função dos MOS humanos num subconjunto anotado. Este modelo é então utilizado para gerar pseudo-MOS para o conjunto completo de dados. Estes rótulos supervisionam depois um regressor profundo sem referência baseado em características extraídas da ResNet-18, originando uma métrica alinhada com a perceção humana que estima a qualidade diretamente a partir de imagens faciais degradadas. Testámos a nossa abordagem no contexto de imagens faciais afetadas por esteganografia, demonstrando a sua eficácia em cenários com distorções subtis e anotações humanas limitadas.